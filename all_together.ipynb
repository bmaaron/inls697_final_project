{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_website(url):\n",
    "    \"\"\"\n",
    "    Scrapes the given URL and returns a dictionary with:\n",
    "      - 'title': The <title> text (or a fallback if none is found).\n",
    "      - 'description': The content of the <meta name='description'> tag if available.\n",
    "      - 'snippet': The first 250 characters of the visible page text.\n",
    "      \n",
    "    Parameters:\n",
    "        url (str): The URL of the website to scrape.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains 'title', 'description', and 'snippet' on success; \n",
    "              or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raises an HTTPError if the status is 4xx/5xx\n",
    "        \n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Get <title>\n",
    "        title_tag = soup.find('title')\n",
    "        if title_tag:\n",
    "            title = title_tag.get_text().strip()\n",
    "        else:\n",
    "            title = \"No title found\"\n",
    "        \n",
    "        # Get <meta name=\"description\" ... >\n",
    "        description_tag = soup.find('meta', attrs={'name': 'description'})\n",
    "        if description_tag and 'content' in description_tag.attrs:\n",
    "            description = description_tag['content'].strip()\n",
    "        else:\n",
    "            description = \"No description found\"\n",
    "        \n",
    "        # Get all visible text\n",
    "        full_text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        # Extract first 250 characters\n",
    "        snippet = full_text[:10000]\n",
    "        \n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"description\": description,\n",
    "            \"snippet\": snippet\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}. Reason: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    url = input(\"Enter URL:\")\n",
    "    result = scrape_website(url)\n",
    "    if result:\n",
    "        print(\"Title:\", result[\"title\"])\n",
    "        print(\"Description:\", result[\"description\"])\n",
    "        print(\"Snippet:\", result[\"snippet\"])\n",
    "    else:\n",
    "        print(\"Scraping failed.\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape https://www.reuters.com/world/middle-east/israeli-military-conducts-strikes-hamas-targets-gaza-army-says-2025-03-18/. Reason: 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/world/middle-east/israeli-military-conducts-strikes-hamas-targets-gaza-army-says-2025-03-18/\n",
      "None\n",
      "Scraping failed for URL: https://www.reuters.com/world/middle-east/israeli-military-conducts-strikes-hamas-targets-gaza-army-says-2025-03-18/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape_website(url):\n",
    "    \"\"\"\n",
    "    Scrapes the given URL and returns a dictionary with:\n",
    "      - 'title': The <title> text (or a fallback if none is found).\n",
    "      - 'description': The content of the <meta name='description'> tag if available.\n",
    "      - 'snippet': The first 10,000 characters of the visible page text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Get <title>\n",
    "        title_tag = soup.find('title')\n",
    "        title = title_tag.get_text().strip() if title_tag else \"No title found\"\n",
    "        \n",
    "        # Get <meta name=\"description\">\n",
    "        description_tag = soup.find('meta', attrs={'name': 'description'})\n",
    "        description = description_tag['content'].strip() if description_tag and 'content' in description_tag.attrs else \"No description found\"\n",
    "        \n",
    "        # Get all visible text\n",
    "        full_text = soup.get_text(separator=' ', strip=True)\n",
    "        snippet = full_text[:10000]  # Adjust as needed\n",
    "        \n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"description\": description,\n",
    "            \"snippet\": snippet\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}. Reason: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load your model and tokenizer.\n",
    "# Replace model_path with your actual model directory.\n",
    "model_path = \"/Users/bryce/Desktop/INLS697/INLS697_proj/DeBERTa/final_model_checkpoint\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Create a text-classification pipeline\n",
    "nlp = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def classify_scraped_text(url):\n",
    "    \"\"\"\n",
    "    Scrapes the URL, prepares the text by combining title, description, and snippet,\n",
    "    then uses the classification pipeline to get the predicted label and confidence.\n",
    "    \"\"\"\n",
    "    data = scrape_website(url)\n",
    "    print(data)\n",
    "    if data is None:\n",
    "        print(\"Scraping failed for URL:\", url)\n",
    "        return\n",
    "    \n",
    "    # Combine parts of the scraped text. Adjust which parts to include as needed.\n",
    "    text_to_classify = f\"{data['title']}\\n{data['description']}\\n{data['snippet']}\"\n",
    "    \n",
    "    # If necessary, truncate the text to avoid input length issues.\n",
    "    # For example, keep only the first 512 tokens if your model has a 512-token limit.\n",
    "    # Here we assume the pipeline handles tokenization appropriately.\n",
    "    \n",
    "    results = nlp(text_to_classify)\n",
    "    \n",
    "    # Print the results. Each result dict contains keys \"label\" and \"score\".\n",
    "    for res in results:\n",
    "        print(\"Label:\", res[\"label\"])\n",
    "        print(\"Confidence:\", res[\"score\"])\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = input(\"Enter URL to classify: \")\n",
    "    classify_scraped_text(url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
